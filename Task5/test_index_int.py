#!/usr/bin/env python3
"""
–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
"""

import os
import time
from sentence_transformers import SentenceTransformer
import chromadb

def load_embedding_model():
    try:
        model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
        print("‚úÖ –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        return model
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        return None

def connect_vector_db():
    try:
        client = chromadb.PersistentClient(path="vector_index")
        collection = client.get_collection("knowledge_base")
        print(f"‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∞. –ß–∞–Ω–∫–æ–≤: {collection.count()}")
        return collection
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–µ: {e}")
        return None

def search_query(collection, embed_model, query, top_k=5):
    try:
        embedding = embed_model.encode([query]).tolist()
        results = collection.query(
            query_embeddings=embedding,
            n_results=top_k,
            include=["documents", "metadatas", "distances"]
        )

        documents = results.get("documents", [[]])[0]
        metadatas = results.get("metadatas", [[]])[0]
        distances = results.get("distances", [[]])[0]

        return list(zip(documents, metadatas, distances))
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
        return []

def interactive_test():
    print("üß™ –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –í–ï–ö–¢–û–†–ù–û–ì–û –ò–ù–î–ï–ö–°–ê")
    print("–î–ª—è –≤—ã—Ö–æ–¥–∞ –≤–≤–µ–¥–∏—Ç–µ 'exit' –∏–ª–∏ Ctrl+C")
    print("=" * 80)

    embed_model = load_embedding_model()
    if not embed_model:
        return

    collection = connect_vector_db()
    if not collection:
        return

    while True:
        try:
            query = input("\nüîç –í–∞—à –∑–∞–ø—Ä–æ—Å: ").strip()
            if query.lower() in ['exit', 'quit']:
                print("üëã –í—ã—Ö–æ–¥ –∏–∑ —Ä–µ–∂–∏–º–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.")
                break

            start_time = time.time()
            results = search_query(collection, embed_model, query)
            elapsed = time.time() - start_time

            if not results:
                print("‚ö†Ô∏è –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
                continue

            print(f"\nüîé –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ (–∑–∞ {elapsed:.2f} —Å–µ–∫):")
            for i, (doc, meta, dist) in enumerate(results, 1):
                source = meta.get("source", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")
                preview = doc[:200].replace("\n", " ") + ("..." if len(doc) > 200 else "")
                print(f"\nüìÑ #{i}:")
                print(f"   –ò—Å—Ç–æ—á–Ω–∏–∫: {source}")
                print(f"   –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {dist:.4f}")
                print(f"   –§—Ä–∞–≥–º–µ–Ω—Ç: {preview}")

        except KeyboardInterrupt:
            print("\nüëã –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
            break
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    interactive_test()
