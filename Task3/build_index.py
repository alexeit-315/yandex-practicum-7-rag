#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
"""

import os
import time
import re
import shutil
import argparse
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
import chromadb
import numpy as np

def batch_data(data, batch_size=4000):
    """–†–∞–∑–±–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –Ω–∞ –±–∞—Ç—á–∏"""
    for i in range(0, len(data), batch_size):
        yield data[i:i + batch_size]

def preprocess_text(text):
    """–û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^\w\s–∞-—è–ê-–Ø—ë–Å\-_.,!?;:]', ' ', text)
    return text.strip()

def load_embedding_model(model_path=None, model_name=None):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ø—É—Ç–∏"""
    try:
        if model_path and os.path.exists(model_path):
            print(f"   üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏–∑: {model_path}")
            model = SentenceTransformer(model_path)
            # –ü–æ–ª—É—á–∞–µ–º –∏–º—è –º–æ–¥–µ–ª–∏ –∏–∑ –ø—É—Ç–∏ –∏–ª–∏ –∫–æ–Ω—Ñ–∏–≥–∞
            model._model_name = os.path.basename(model_path)
            return model
        elif model_name:
            print(f"   üåê –ó–∞–≥—Ä—É–∑–∫–∞ –æ–Ω–ª–∞–π–Ω –º–æ–¥–µ–ª–∏: {model_name}")
            model = SentenceTransformer(model_name)
            model._model_name = model_name
            return model
        else:
            # –†–µ–∑–µ—Ä–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
            models_to_try = [
                "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                "sentence-transformers/all-MiniLM-L6-v2"
            ]

            for model_name in models_to_try:
                try:
                    print(f"   üîÑ –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å: {model_name}")
                    model = SentenceTransformer(model_name)
                    model._model_name = model_name
                    return model
                except Exception as e:
                    print(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {model_name}: {e}")
                    continue

            raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω—É –º–æ–¥–µ–ª—å")

    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        return None

def create_vector_index(model_path=None, model_name=None, chunk_size=384):
    """–°–æ–∑–¥–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å —Å —É–∫–∞–∑–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é"""

    print("üîç –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
    embed_model = load_embedding_model(model_path, model_name)

    if not embed_model:
        return None

    model_name = getattr(embed_model, '_model_name', 'Unknown')
    print(f"   ‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_name}")
    print(f"   üìä –†–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {embed_model.get_sentence_embedding_dimension()} –∏–∑–º–µ—Ä–µ–Ω–∏–π")

    print("\nüìÑ –®–∞–≥ 2: –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
    source_folder = "knowledge_base"

    if not os.path.exists(source_folder):
        print(f"   ‚ùå –û—à–∏–±–∫–∞: –ü–∞–ø–∫–∞ '{source_folder}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
        return None

    text_files = [f for f in os.listdir(source_folder) if f.endswith(('.txt', '.md'))]
    print(f"   üìÅ –ù–∞–π–¥–µ–Ω–æ {len(text_files)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

    all_chunks = []
    chunks_metadatas = []

    for filename in text_files:
        filepath = os.path.join(source_folder, filename)
        try:
            with open(filepath, 'r', encoding='utf-8') as file:
                content = file.read()

            content = preprocess_text(content)
            title = os.path.splitext(filename)[0].replace('_', ' ')

            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=50,
                length_function=len,
                separators=["\n\n", "\n", ". ", "! ", "? ", "; ", ", ", " "],
                add_start_index=True,
            )

            chunks = text_splitter.create_documents([content])

            for i, chunk in enumerate(chunks):
                enhanced_content = f"–î–æ–∫—É–º–µ–Ω—Ç: {title}\n–¢–µ–º–∞: {title}\n\n{chunk.page_content}"

                all_chunks.append(enhanced_content)
                chunks_metadatas.append({
                    "source": filename,
                    "title": title,
                    "chunk_id": i,
                    "start_index": chunk.metadata.get('start_index', 0),
                    "content_length": len(chunk.page_content)
                })

        except Exception as e:
            print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {filename}: {e}")

    print(f"   ‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(all_chunks)} —á–∞–Ω–∫–æ–≤")

    print("\nüßÆ –®–∞–≥ 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
    start_time = time.time()

    try:
        chunk_embeddings = embed_model.encode(
            all_chunks,
            show_progress_bar=True,
            batch_size=16,
            convert_to_numpy=True,
            normalize_embeddings=True,
            device='cpu'
        )

        embedding_time = time.time() - start_time
        print(f"   ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã –∑–∞ {embedding_time:.2f} —Å–µ–∫—É–Ω–¥")

    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
        return None

    print("\nüíæ –®–∞–≥ 4: –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞...")
    persist_directory = "vector_index"

    if os.path.exists(persist_directory):
        try:
            shutil.rmtree(persist_directory)
            print("   üóëÔ∏è –£–¥–∞–ª–µ–Ω —Å—Ç–∞—Ä—ã–π –∏–Ω–¥–µ–∫—Å")
        except:
            pass

    try:
        client = chromadb.PersistentClient(path=persist_directory)

        collection = client.get_or_create_collection(
            name="knowledge_base",
            metadata={
                "hnsw:space": "cosine",
                "model": model_name,
                "chunk_size": str(chunk_size),
                "embedding_dim": str(embed_model.get_sentence_embedding_dimension())
            }
        )

        # –ë–∞—Ç—á–∏–Ω–≥
        batch_size = 3500
        total_batches = (len(all_chunks) + batch_size - 1) // batch_size

        print(f"   üì¶ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö ({total_batches} –±–∞—Ç—á–µ–π)...")

        for batch_num, (batch_indices, batch_embeddings, batch_metadatas, batch_documents) in enumerate(
            zip(
                batch_data(list(range(len(all_chunks))), batch_size),
                batch_data(chunk_embeddings.tolist(), batch_size),
                batch_data(chunks_metadatas, batch_size),
                batch_data(all_chunks, batch_size)
            )
        ):
            print(f"   üîÑ –ë–∞—Ç—á {batch_num + 1}/{total_batches} ({len(batch_indices)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤)")

            batch_ids = [f"chunk_{i}" for i in batch_indices]

            collection.add(
                embeddings=batch_embeddings,
                metadatas=batch_metadatas,
                documents=batch_documents,
                ids=batch_ids
            )

        print(f"   ‚úÖ –í–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ '{persist_directory}/'")

        return {
            "client": client,
            "collection": collection,
            "embed_model": embed_model,
            "chunk_count": len(all_chunks),
            "embedding_time": embedding_time,
            "model_name": model_name
        }

    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞: {e}")
        return None

def interactive_search(collection, embed_model):
    """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫"""
    print("\n" + "="*80)
    print("üîç –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –ü–û–ò–°–ö")
    print("="*80)

    while True:
        try:
            query = input("\nüéØ –í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å (–∏–ª–∏ 'quit' –¥–ª—è –≤—ã—Ö–æ–¥–∞): ").strip()

            if query.lower() in ['quit', 'exit', 'q']:
                break
            elif not query:
                continue

            start_time = time.time()
            query_embedding = embed_model.encode([query]).tolist()

            results = collection.query(
                query_embeddings=query_embedding,
                n_results=5,
                include=["metadatas", "documents", "distances"]
            )

            search_time = time.time() - start_time

            if results['documents'] and results['documents'][0]:
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results['documents'][0])} (–≤—Ä–µ–º—è: {search_time:.3f}—Å)")
                print("=" * 70)

                for i, (doc, metadata, distance) in enumerate(zip(
                    results['documents'][0],
                    results['metadatas'][0],
                    results['distances'][0]
                )):
                    print(f"\nüèÜ –†–ï–ó–£–õ–¨–¢–ê–¢ {i+1} (–∫–∞—á–µ—Å—Ç–≤–æ: {1-distance:.3f})")
                    print(f"üìÑ –î–æ–∫—É–º–µ–Ω—Ç: {metadata.get('title', 'N/A')}")
                    print(f"üìÅ –§–∞–π–ª: {metadata.get('source', 'N/A')}")

                    content_start = doc.find('\n\n') + 2
                    content = doc[content_start:] if content_start > 2 else doc
                    snippet = content[:200] + "..." if len(content) > 200 else content

                    print(f"üìù –°–Ω–∏–ø–ø–µ—Ç: {snippet}")
                    print(f"üìê –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {distance:.4f}")
                    print("-" * 50)
            else:
                print("‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")

def main():
    parser = argparse.ArgumentParser(description="–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞")
    parser.add_argument("--model-path", help="–ü—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏")
    parser.add_argument("--model-name", help="–ù–∞–∑–≤–∞–Ω–∏–µ –æ–Ω–ª–∞–π–Ω –º–æ–¥–µ–ª–∏")
    parser.add_argument("--chunk-size", type=int, default=384, help="–†–∞–∑–º–µ—Ä —á–∞–Ω–∫–æ–≤")
    parser.add_argument("--no-interactive", action="store_true", help="–ù–µ –∑–∞–ø—É—Å–∫–∞—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫")

    args = parser.parse_args()

    print("="*80)
    print("üõ†Ô∏è  –°–û–ó–î–ê–ù–ò–ï –í–ï–ö–¢–û–†–ù–û–ì–û –ò–ù–î–ï–ö–°–ê")
    print("="*80)

    if os.path.exists("vector_index"):
        response = input("–ò–Ω–¥–µ–∫—Å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å? (y/N): ").strip().lower()
        if response != 'y':
            print("–ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∏–Ω–¥–µ–∫—Å–∞...")
            try:
                client = chromadb.PersistentClient(path="vector_index")
                collection = client.get_collection("knowledge_base")
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–∏—Å–∫–∞
                embed_model = load_embedding_model(args.model_path, args.model_name)
                if embed_model and not args.no_interactive:
                    interactive_search(collection, embed_model)
                return
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞: {e}")

    result = create_vector_index(args.model_path, args.model_name, args.chunk_size)

    if result:
        print("\n" + "="*80)
        print("‚úÖ –ò–ù–î–ï–ö–° –£–°–ü–ï–®–ù–û –°–û–ó–î–ê–ù!")
        print("="*80)
        print(f"ü§ñ –ú–æ–¥–µ–ª—å: {result['model_name']}")
        print(f"üì¶ –ß–∞–Ω–∫–æ–≤: {result['chunk_count']}")
        print(f"‚è±Ô∏è –í—Ä–µ–º—è: {result['embedding_time']:.2f} —Å–µ–∫—É–Ω–¥")

        if not args.no_interactive:
            interactive_search(result['collection'], result['embed_model'])
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å")

if __name__ == "__main__":
    main()