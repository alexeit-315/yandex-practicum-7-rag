#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
"""

import os
import time
import json
from sentence_transformers import SentenceTransformer
import chromadb

def load_embedding_model(model_path=None, model_name=None):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    try:
        if model_path and os.path.exists(model_path):
            model = SentenceTransformer(model_path)
            model._model_name = os.path.basename(model_path)
            return model
        elif model_name:
            model = SentenceTransformer(model_name)
            model._model_name = model_name
            return model
        else:
            model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
            model._model_name = "paraphrase-multilingual-MiniLM-L12-v2"
            return model
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        return None

def test_search(collection, embed_model, query, expected_files=None, n_results=5):
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
    start_time = time.time()

    try:
        query_embedding = embed_model.encode([query]).tolist()

        results = collection.query(
            query_embeddings=query_embedding,
            n_results=n_results,
            include=["metadatas", "documents", "distances"]
        )

        search_time = time.time() - start_time

        found_files = []
        if results['metadatas'] and results['metadatas'][0]:
            found_files = [meta['source'] for meta in results['metadatas'][0]]

        # –†–∞—Å—á–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏
        precision = 0
        if expected_files and found_files:
            relevant = sum(1 for file in found_files if any(exp in file for exp in expected_files))
            precision = relevant / len(found_files)

        return {
            "query": query,
            "time": search_time,
            "results_count": len(found_files),
            "found_files": found_files,
            "precision": precision,
            "distances": results['distances'][0] if results['distances'] else [],
            "success": True
        }

    except Exception as e:
        return {
            "query": query,
            "error": str(e),
            "success": False
        }

def run_comprehensive_test():
    """–ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""

    print("="*80)
    print("üß™ –ö–û–ú–ü–õ–ï–ö–°–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –í–ï–ö–¢–û–†–ù–û–ì–û –ò–ù–î–ï–ö–°–ê")
    print("="*80)

    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –∏–Ω–¥–µ–∫—Å–∞
    embed_model = load_embedding_model()
    if not embed_model:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å")
        return

    if not os.path.exists("vector_index"):
        print("‚ùå –í–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return

    try:
        client = chromadb.PersistentClient(path="vector_index")
        collection = client.get_collection("knowledge_base")

        print(f"‚úÖ –ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω. –ß–∞–Ω–∫–æ–≤: {collection.count()}")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞: {e}")
        return

    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã —Å –æ–∂–∏–¥–∞–µ–º—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    test_cases = [
        # –ü–µ—Ä—Å–æ–Ω–∞–∂–∏
        {"query": "–ö—Ä—ã—à –®–∫–∞–π–∑—é–∫—ë—Ä", "expected": ["–ö—Ä—ã—à_–®–∫–∞–π–∑—é–∫—ë—Ä.txt"]},
        {"query": "–©—ã–± –®—É—Ä—Ä—É–º—Ö–µ—Ä", "expected": ["–©—ã–±_–®—É—Ä—Ä—É–º—Ö–µ—Ä.txt"]},
        {"query": "–û–±–∏-–î–≤–∞-–í–∞–Ω–∏ –ö–∏–Ω—É—Ä–∏", "expected": ["–û–±–∏-–î–≤–∞-–í–∞–Ω–∏_–ö–∏–Ω—É—Ä–∏.txt"]},
        {"query": "–õ—ë—è –û—Ä–≥–∞–Ω–∞", "expected": ["–õ—ë—è_–û—Ä–≥–∞–Ω–∞.txt"]},
        {"query": "–£–Ω–∞–∫—ã–Ω –®–∫–∞–π–∑—é–∫—ë—Ä", "expected": ["–£–Ω–∞–∫—ã–Ω_–®–∫–∞–π–∑—é–∫—ë—Ä.txt"]},

        # –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
        {"query": "–ì–∞–ª–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –ù–∞—Ä–æ–¥–æ–≤–ª–∞—Å—Ç–∞", "expected": ["–ì–∞–ª–∞–∫—Ç–∏—á–µ—Å–∫–∞—è_–ù–∞—Ä–æ–¥–æ–≤–ª–∞—Å—Ç–∞.txt"]},
        {"query": "–û—Ä–¥–µ–Ω –∑—ë–Ω–∑—é–º–∞–µ–≤", "expected": ["–û—Ä–¥–µ–Ω_–∑—ë–Ω–∑—é–º–∞–µ–≤.txt"]},
        {"query": "–§—ã—Ä—Ö–∏", "expected": ["–§—ã—Ä—Ö–∏.txt", "–ì–∞—Ä—Ç_–ü–ª—É–¥–∞—Ñ.txt", "–ì–∞—Ä—Ç_–ú–æ–ª.txt"]},
        {"query": "–®–∞—Ö–∏–Ω—à–∞—Ö–∏—è", "expected": ["–®–∞—Ö–∏–Ω—à–∞—Ö–∏—è.txt", "–©—ã–±_–®—É—Ä—Ä—É–º—Ö–µ—Ä.txt"]},

        # –°–æ–±—ã—Ç–∏—è –∏ –±–∏—Ç–≤—ã
        {"query": "–ó—ë–Ω–∑—é–º–∞–π—Å–∫–æ-—Ñ—ã—Ä—Ö—Å–∫–∞—è –≤–æ–π–Ω–∞", "expected": ["–ó—ë–Ω–∑—é–º–∞–π—Å–∫–æ-—Ñ—ã—Ä—Ö—Å–∫–∞—è_–≤–æ–π–Ω–∞.txt"]},
        {"query": "–ë–∏—Ç–≤–∞ –ø—Ä–∏ –£–Ω–≥—é—Ä–µ", "expected": ["–ë–∏—Ç–≤–∞_–ø—Ä–∏_–£–Ω–≥—é—Ä–µ.txt"]},
        {"query": "–ë–∏—Ç–≤–∞ –ø—Ä–∏ –ê–±—ã—Ä–≤–∞–ª–≥–µ", "expected": ["–ë–∏—Ç–≤–∞_–ø—Ä–∏_–ê–±—ã—Ä–≤–∞–ª–≥–µ.txt"]},
        {"query": "–í–æ–π–Ω—ã –∫–ª–æ–Ω–æ–≤", "expected": ["–í–æ–π–Ω—ã_–∫–ª–æ–Ω–æ–≤.txt"]},

        # –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ –æ–±—ä–µ–∫—Ç—ã
        {"query": "SKEX —Å–ø–∏–¥–µ—Ä", "expected": ["SKEX_—Å–ø–∏–¥–µ—Ä.txt"]},
        {"query": "–ó–≤–µ–∑–¥–∞ –®—Ä–µ–∫–ª–∏—Ö–µ—Ä—Ç–æ–¥–∞", "expected": ["–ó–≤–µ–∑–¥–∞_–®—Ä–µ–∫–ª–∏—Ö–µ—Ä—Ç–æ–¥–∞_I.txt"]},
        {"query": "–°–≤–µ—Ç–æ–≤–æ–π –º–µ—á", "expected": ["–°–±–æ—Ä–∫–∞_—Å–≤–µ—Ç–æ–≤–æ–≥–æ_–º–µ—á–∞.txt"]},
        {"query": "–î—Ä–æ–∏–¥", "expected": ["–î—Ä–æ–∏–¥.txt", "Eleganz.txt", "U6-B7.txt"]},

        # –ü–ª–∞–Ω–µ—Ç—ã –∏ –º–µ—Å—Ç–∞
        {"query": "–°—ë–ª—ç—á–∏—è", "expected": ["–°—ë–ª—ç—á–∏—è.txt"]},
        {"query": "–ß–∞—Ç—ç–∏–Ω", "expected": ["–ß–∞—Ç—ç–∏–Ω.txt", "–£–Ω–∞–∫—ã–Ω_–®–∫–∞–π–∑—é–∫—ë—Ä.txt"]},
        {"query": "–ù–∞–≤—ç", "expected": ["–ù–∞–≤—ç.txt", "–ü—Ä–∞–≥–º–µ_–®–º—ã–≥–∞–ª–∞.txt"]},
        {"query": "–ê–±—ã—Ä–≤–∞–ª–≥", "expected": ["–ê–±—ã—Ä–≤–∞–ª–≥.txt", "–ë–∏—Ç–≤–∞_–ø—Ä–∏_–ê–±—ã—Ä–≤–∞–ª–≥–µ.txt"]},
    ]

    print(f"\nüìä –ó–∞–ø—É—Å–∫ {len(test_cases)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤...")
    print("-" * 80)

    results = []
    total_precision = 0
    total_time = 0
    successful_tests = 0

    for i, test_case in enumerate(test_cases, 1):
        print(f"üß™ –¢–µ—Å—Ç {i}/{len(test_cases)}: '{test_case['query']}'")

        result = test_search(collection, embed_model, test_case['query'], test_case['expected'])

        if result['success']:
            status = "‚úÖ" if result['precision'] > 0.5 else "‚ö†Ô∏è" if result['precision'] > 0 else "‚ùå"
            print(f"   {status} –¢–æ—á–Ω–æ—Å—Ç—å: {result['precision']:.3f} | –í—Ä–µ–º—è: {result['time']:.3f}—Å")
            print(f"   üìÅ –ù–∞–π–¥–µ–Ω–æ: {', '.join(result['found_files'][:3])}{'...' if len(result['found_files']) > 3 else ''}")

            total_precision += result['precision']
            total_time += result['time']
            successful_tests += 1
        else:
            print(f"   ‚ùå –û—à–∏–±–∫–∞: {result['error']}")

        results.append(result)
        print()

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    if successful_tests > 0:
        avg_precision = total_precision / successful_tests
        avg_time = total_time / successful_tests

        print("="*80)
        print("üìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
        print("="*80)
        print(f"‚úÖ –£—Å–ø–µ—à–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤: {successful_tests}/{len(test_cases)}")
        print(f"üéØ –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å: {avg_precision:.3f}")
        print(f"‚è±Ô∏è –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞: {avg_time:.3f}—Å")

        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        excellent = sum(1 for r in results if r.get('precision', 0) > 0.8)
        good = sum(1 for r in results if 0.5 < r.get('precision', 0) <= 0.8)
        poor = sum(1 for r in results if 0 < r.get('precision', 0) <= 0.5)
        failed = len(results) - excellent - good - poor

        print(f"\nüìä –ö–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
        print(f"   üèÜ –û—Ç–ª–∏—á–Ω–æ (>0.8): {excellent}")
        print(f"   üëç –•–æ—Ä–æ—à–æ (0.5-0.8): {good}")
        print(f"   ‚ö†Ô∏è –ü–ª–æ—Ö–æ (0-0.5): {poor}")
        print(f"   ‚ùå –û—à–∏–±–∫–∏: {failed}")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        results_file = f"test_results_{timestamp}.json"

        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump({
                "timestamp": timestamp,
                "total_tests": len(test_cases),
                "successful_tests": successful_tests,
                "average_precision": avg_precision,
                "average_time": avg_time,
                "results": results
            }, f, ensure_ascii=False, indent=2)

        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {results_file}")

    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞")

def quick_test():
    """–ë—ã—Å—Ç—Ä–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""

    print("üöÄ –ë–´–°–¢–†–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï")
    print("-" * 40)

    embed_model = load_embedding_model()
    if not embed_model or not os.path.exists("vector_index"):
        print("‚ùå –ú–æ–¥–µ–ª—å –∏–ª–∏ –∏–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return

    try:
        client = chromadb.PersistentClient(path="vector_index")
        collection = client.get_collection("knowledge_base")

        quick_queries = [
            "–ö—Ä—ã—à –®–∫–∞–π–∑—é–∫—ë—Ä",
            "–ì–∞–ª–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –ù–∞—Ä–æ–¥–æ–≤–ª–∞—Å—Ç–∞",
            "–ó–≤–µ–∑–¥–∞ –®—Ä–µ–∫–ª–∏—Ö–µ—Ä—Ç–æ–¥–∞",
            "–°–ö–ï–• —Å–ø–∏–¥–µ—Ä",
            "–©—ã–± –®—É—Ä—Ä—É–º—Ö–µ—Ä"
        ]

        for query in quick_queries:
            result = test_search(collection, embed_model, query)
            if result['success']:
                status = "‚úÖ" if result['precision'] > 0 else "‚ùå"
                print(f"{status} '{query}' -> {result['precision']:.3f}")
            else:
                print(f"‚ùå '{query}' -> –û—à–∏–±–∫–∞")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞")
    parser.add_argument("--quick", action="store_true", help="–ë—ã—Å—Ç—Ä–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ")
    parser.add_argument("--model-path", help="–ü—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏")
    parser.add_argument("--model-name", help="–ù–∞–∑–≤–∞–Ω–∏–µ –æ–Ω–ª–∞–π–Ω –º–æ–¥–µ–ª–∏")

    args = parser.parse_args()

    if args.quick:
        quick_test()
    else:
        run_comprehensive_test()