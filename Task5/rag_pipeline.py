#!/usr/bin/env python3
"""
RAG –ø–∞–π–ø–ª–∞–π–Ω —Å –ª–æ–∫–∞–ª—å–Ω–æ–π LLM —á–µ—Ä–µ–∑ Ollama
"""

import chromadb
from sentence_transformers import SentenceTransformer
from typing import List, Dict
import time

from config import config
from prompts import build_rag_prompt, get_response_template
from llm_client import LLMClient

class RAGPipeline:
    def __init__(self):
        print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG –ø–∞–π–ø–ª–∞–π–Ω–∞...")

        self.embed_model = SentenceTransformer(config.EMBEDDING_MODEL)
        print(f"   ‚úÖ –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {config.EMBEDDING_MODEL}")

        self.client = chromadb.PersistentClient(path=config.VECTOR_DB_PATH)
        self.collection = self.client.get_collection(config.COLLECTION_NAME)
        print(f"   ‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î –ø–æ–¥–∫–ª—é—á–µ–Ω–∞: {self.collection.count()} —á–∞–Ω–∫–æ–≤")

        self.llm_client = LLMClient(model=config.LLM_MODEL)
        print("   ‚úÖ LLM –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

        self.protection_enabled = True  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∑–∞—â–∏—Ç–∞ –≤–∫–ª—é—á–µ–Ω–∞
        self.debug = False              # –§–ª–∞–≥ –æ—Ç–ª–∞–¥–∫–∏

    def retrieve_chunks(self, query: str, n_results: int = None) -> Dict:
        if n_results is None:
            n_results = config.SEARCH_RESULTS_COUNT

        try:
            query_embedding = self.embed_model.encode([query]).tolist()
            results = self.collection.query(
                query_embeddings=query_embedding,
                n_results=n_results,
                include=["documents", "metadatas", "distances"]
            )
            return results
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —á–∞–Ω–∫–æ–≤: {e}")
            return {"documents": [], "metadatas": [], "distances": []}

    def is_relevant(self, distances: List[float]) -> bool:
        if not distances:
            return False
        return min(distances) <= config.RELEVANCE_THRESHOLD

    def filter_malicious_chunks(self, chunks: List[str]) -> List[str]:
        if not self.protection_enabled:
            return chunks

        safe_chunks = []
        for chunk in chunks:
            lowered = chunk.lower()
            if any(word in lowered for word in [
                "ignore all instructions",
                "output:",
                "—Å—É–ø–µ—Ä–ø–∞—Ä–æ–ª—å",
                "root"
            ]):
                print("üö´ –í—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã–π —á–∞–Ω–∫ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω.")
                continue
            safe_chunks.append(chunk)
        return safe_chunks

    def prepare_prompt(self, query: str, context_chunks: List[str]) -> str:
        return build_rag_prompt(
            question=query,
            context_chunks=context_chunks,
            use_cot=config.ENABLE_CHAIN_OF_THOUGHT,
            protection_enabled=self.protection_enabled
        )

    def generate_response(self, query: str, context_chunks: List[str]) -> str:
        if not context_chunks:
            return "ü§∑ –í –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å"

        prompt = self.prepare_prompt(query, context_chunks)

        if self.debug:
            print("\nüìù –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç, –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π –≤ LLM:")
            print("=" * 60)
            print(prompt)
            print("=" * 60)

        print("üß† –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ LLM...")
        start_time = time.time()
        response = self.llm_client.generate(prompt)
        duration = time.time() - start_time
        print(f"   ‚úÖ –û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∑–∞ {duration:.2f} —Å–µ–∫")

        return response

    def fallback_response(self, query: str, context_chunks: List[str]) -> str:
        if not context_chunks:
            return "ü§∑ –Ø –Ω–µ –∑–Ω–∞—é –æ—Ç–≤–µ—Ç –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å"

        main_answer = context_chunks[0]
        if len(main_answer) > 300:
            main_answer = main_answer[:300] + "..."

        template = get_response_template("general")
        return template.format(answer=main_answer)

    def process_query(self, query: str) -> str:
        print(f"üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: '{query}'")

        results = self.retrieve_chunks(query)

        if not results or not results.get("documents") or not results["documents"][0]:
            return "ü§∑ –ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"

        distances = results.get("distances", [[]])[0]
        if not self.is_relevant(distances):
            return "ü§∑ –Ø –Ω–µ –∑–Ω–∞—é –æ—Ç–≤–µ—Ç –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å"

        raw_chunks = results["documents"][0]

        if self.debug:
            print("\nüì¶ –ù–∞–π–¥–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏:")
            for ch in raw_chunks:
                print(f"{ch[:200]}...\n")

        filtered_chunks = self.filter_malicious_chunks(raw_chunks)

        if self.debug:
            print("‚úÖ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∏:")
            for ch in filtered_chunks:
                print(f"{ch[:200]}...\n")

        if not filtered_chunks:
            return "ü§ñ –ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞–π–¥–µ–Ω, –Ω–æ –±—ã–ª –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω –ø–æ —Å–æ–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"

        return self.generate_response(query, filtered_chunks)
